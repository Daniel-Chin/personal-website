<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:w="urn:schemas-microsoft-com:office:word"
xmlns:m="http://schemas.microsoft.com/office/2004/12/omml"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=ProgId content=Word.Document>
<meta name=Generator content="Microsoft Word 15">
<meta name=Originator content="Microsoft Word 15">
<link rel=File-List href="biLearn_files/filelist.xml">
<link rel=themeData href="biLearn_files/themedata.thmx">
<link rel=colorSchemeMapping href="biLearn_files/colorschememapping.xml">

<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;
	mso-font-charset:0;
	mso-generic-font-family:roman;
	mso-font-pitch:variable;
	mso-font-signature:3 0 0 0 1 0;}
@font-face
	{font-family:等线;
	panose-1:2 1 6 0 3 1 1 1 1 1;
	mso-font-alt:DengXian;
	mso-font-charset:134;
	mso-generic-font-family:auto;
	mso-font-pitch:variable;
	mso-font-signature:-1610612033 953122042 22 0 262159 0;}
@font-face
	{font-family:Verdana;
	panose-1:2 11 6 4 3 5 4 4 2 4;
	mso-font-charset:0;
	mso-generic-font-family:swiss;
	mso-font-pitch:variable;
	mso-font-signature:-1610610945 1073750107 16 0 415 0;}
@font-face
	{font-family:"\@等线";
	panose-1:2 1 6 0 3 1 1 1 1 1;
	mso-font-charset:134;
	mso-generic-font-family:auto;
	mso-font-pitch:variable;
	mso-font-signature:-1610612033 953122042 22 0 262159 0;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-unhide:no;
	mso-style-qformat:yes;
	mso-style-parent:"";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	line-height:107%;
	mso-pagination:widow-orphan;
	font-size:11.0pt;
	font-family:"Verdana",sans-serif;
	mso-fareast-font-family:等线;
	mso-fareast-theme-font:minor-fareast;
	mso-bidi-font-family:"Times New Roman";
	mso-bidi-theme-font:minor-bidi;}
h2
	{mso-style-priority:9;
	mso-style-qformat:yes;
	mso-style-link:"Heading 2 Char";
	mso-style-next:Normal;
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	line-height:107%;
	mso-pagination:widow-orphan lines-together;
	page-break-after:avoid;
	mso-outline-level:2;
	font-size:13.0pt;
	font-family:"Verdana",sans-serif;
	mso-fareast-font-family:"等线 Light";
	mso-fareast-theme-font:major-fareast;
	mso-bidi-font-family:"Times New Roman";
	mso-bidi-theme-font:major-bidi;
	color:#2F5496;
	mso-themecolor:accent1;
	mso-themeshade:191;
	font-weight:normal;}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-noshow:yes;
	mso-style-priority:99;
	mso-style-link:"Balloon Text Char";
	margin:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	mso-pagination:widow-orphan;
	font-size:9.0pt;
	font-family:"Times New Roman",serif;
	mso-fareast-font-family:等线;
	mso-fareast-theme-font:minor-fareast;}
span.Heading2Char
	{mso-style-name:"Heading 2 Char";
	mso-style-priority:9;
	mso-style-unhide:no;
	mso-style-locked:yes;
	mso-style-link:"Heading 2";
	mso-ansi-font-size:13.0pt;
	mso-bidi-font-size:13.0pt;
	font-family:"Verdana",sans-serif;
	mso-ascii-font-family:Verdana;
	mso-fareast-font-family:"等线 Light";
	mso-fareast-theme-font:major-fareast;
	mso-hansi-font-family:Verdana;
	mso-bidi-font-family:"Times New Roman";
	mso-bidi-theme-font:major-bidi;
	color:#2F5496;
	mso-themecolor:accent1;
	mso-themeshade:191;}
span.BalloonTextChar
	{mso-style-name:"Balloon Text Char";
	mso-style-noshow:yes;
	mso-style-priority:99;
	mso-style-unhide:no;
	mso-style-locked:yes;
	mso-style-link:"Balloon Text";
	mso-ansi-font-size:9.0pt;
	mso-bidi-font-size:9.0pt;
	font-family:"Times New Roman",serif;
	mso-ascii-font-family:"Times New Roman";
	mso-hansi-font-family:"Times New Roman";
	mso-bidi-font-family:"Times New Roman";}
.MsoChpDefault
	{mso-style-type:export-only;
	mso-default-props:yes;
	font-family:"Calibri",sans-serif;
	mso-ascii-font-family:Calibri;
	mso-ascii-theme-font:minor-latin;
	mso-fareast-font-family:等线;
	mso-fareast-theme-font:minor-fareast;
	mso-hansi-font-family:Calibri;
	mso-hansi-theme-font:minor-latin;
	mso-bidi-font-family:"Times New Roman";
	mso-bidi-theme-font:minor-bidi;}
.MsoPapDefault
	{mso-style-type:export-only;
	margin-bottom:8.0pt;
	line-height:107%;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:.5in 1.0in .5in 1.0in;
	mso-header-margin:.5in;
	mso-footer-margin:.5in;
	mso-paper-source:0;}
div.WordSection1
	{page:WordSection1;}
-->
</style>
<!--[if gte mso 10]>
<style>
 /* Style Definitions */
 table.MsoNormalTable
	{mso-style-name:"Table Normal";
	mso-tstyle-rowband-size:0;
	mso-tstyle-colband-size:0;
	mso-style-noshow:yes;
	mso-style-priority:99;
	mso-style-parent:"";
	mso-padding-alt:0in 5.4pt 0in 5.4pt;
	mso-para-margin-top:0in;
	mso-para-margin-right:0in;
	mso-para-margin-bottom:8.0pt;
	mso-para-margin-left:0in;
	line-height:107%;
	mso-pagination:widow-orphan;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;
	mso-ascii-font-family:Calibri;
	mso-ascii-theme-font:minor-latin;
	mso-hansi-font-family:Calibri;
	mso-hansi-theme-font:minor-latin;
	mso-bidi-font-family:"Times New Roman";
	mso-bidi-theme-font:minor-bidi;}
</style>
<![endif]--><!--[if gte mso 9]><xml>
 <o:shapedefaults v:ext="edit" spidmax="1026"/>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <o:shapelayout v:ext="edit">
  <o:idmap v:ext="edit" data="1"/>
 </o:shapelayout></xml><![endif]-->
</head>

<body lang=EN-US style='tab-interval:.5in;word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal><span style='font-size:12.0pt;line-height:107%;font-family:
"Times New Roman",serif'>Let me describe to you a long-lasting frustration and
a new hope. <o:p></o:p></span></p>

<p class=MsoNormal><span style='font-size:12.0pt;line-height:107%;font-family:
"Times New Roman",serif'>The current channels of musical expression are like
thin tubes that musical ideas have to squeeze through. A Digital Audio
Workstation (DAW) is expressive but not real-time. A trumpet is real-time but
not expressive. How can a machine extract humans' internal music imagination,
so that a genius like Bach could compose the four parts of a four-part fugue
simultaneously; so that a non-expert could improvise music without mastering
any instrument or DAW? <o:p></o:p></span></p>

<p class=MsoNormal><span style='font-size:12.0pt;line-height:107%;font-family:
"Times New Roman",serif'>Allow me to show you my answer: a dynamically
scaffolded multi-modal <i>co-adaptation</i> between a human and her bespoke instrument
through interactive machine learning. Picture an instrument that translates the
human’s hand gesture, body motions, breathing, micro facial expressions, tongue
movement details, muscle activations, and EEG signal into the music she is
imagining. The instrument uses an ensemble of variational neural networks
supervised by parallel data generated when she listens to or improvises music.
Two training techniques foster better generalization from limited training
data: 1) KL divergence loss ensures <i>low-frequency</i> <i>continuity</i> of
the mapping; 2) Cycle consistency makes the entire output space <i>reachable</i>.
Additionally, the human plays the instrument, marks undesirable behaviors, and
scores the various ensemble learners in offline review sessions. <o:p></o:p></span></p>

<p class=MsoNormal><span style='font-size:12.0pt;line-height:107%;font-family:
"Times New Roman",serif'>Furthermore, the human learns the instrument! Contrary
to a static decoding task where the ground truth is passive, in co-adaptation
the ground truth also moves towards the decoder. Concretely, when the human
sees something the instrument does, she latches onto that and has some
low-dimensional control over the instrument. She tries exposing different
features for the instrument to learn. The human-instrument bi-directional
learning never ends and the dimensionality of control increases. Moreover, the
instrument applies haptic guidance to train the human. Specifically, the human
selects a piece she wants to play, and the haptic ground truth is computed by
the opposite encoder in charge of the cycle consistency. Using haptic guidance,
the human can also review old training data and tell the instrument to "forget"
outdated ones. <o:p></o:p></span></p>

<p class=MsoNormal><span style='font-size:12.0pt;line-height:107%;font-family:
"Times New Roman",serif'>Here is the big picture in my eyes. Mind-reading the
human would offer perfect expressivity, but unintrusive readings like EEG are
unassailably noisy – an unreliable mapping. A piano translates finger motions
to music, but is not nearly as expressive – a collapsed mapping. Co-adaptation
gradually finds a mapping where the human can interface with the instrument at
maximal information throughput. With many users, we can summarize several
&quot;pruned / principal instruments&quot; for beginners to fork and jump start
the training. <o:p></o:p></span></p>

<p class=MsoNormal><span style='font-size:12.0pt;line-height:107%;font-family:
"Times New Roman",serif'>We will finally be free! Non-experts will be able to jam
together in symphony. Musicians will be able to improvise multi-part
novel-timbre music while hearing it in real time! <o:p></o:p></span></p>

<p class=MsoNormal><span style='font-size:12.0pt;line-height:107%;font-family:
"Times New Roman",serif'>Viability-wise, I see various possible first steps.
The initialization of the mapping can be copied from an existing instrument. I
can go from a <i>generator</i> that decodes random noise to music, to a <i>controlled
generator</i> that decodes some noise in junction with human input, and finally
to an <i>instrument</i> that decodes the human. <o:p></o:p></span></p>

</div>

</body>

</html>
